{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipaddress\n",
    "from datetime import timedelta\n",
    "from collections import Counter\n",
    "\n",
    "# For consistent plotting\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# === Utility / Base Classes ===\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Load and perform initial cleaning on required datasets.\"\"\"\n",
    "    def __init__(self, fraud_path, ip_path, credit_path):\n",
    "        self.fraud_path = fraud_path\n",
    "        self.ip_path = ip_path\n",
    "        self.credit_path = credit_path\n",
    "\n",
    "    def load_fraud(self):\n",
    "        df = pd.read_csv(self.fraud_path)\n",
    "        # Timestamp parsing\n",
    "        df['signup_time'] = pd.to_datetime(df['signup_time'], errors='coerce')\n",
    "        df['purchase_time'] = pd.to_datetime(df['purchase_time'], errors='coerce')\n",
    "        # Drop exact duplicates\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def load_ip_country(self):\n",
    "        ip_df = pd.read_csv(self.ip_path)\n",
    "        # Convert the bounds to integers for range matching\n",
    "        ip_df['lower_int'] = ip_df['lower_bound_ip_address'].astype(int)\n",
    "        ip_df['upper_int'] = ip_df['upper_bound_ip_address'].astype(int)\n",
    "        # Keep needed columns\n",
    "        return ip_df[['lower_int', 'upper_int', 'country']].sort_values('lower_int').reset_index(drop=True)\n",
    "\n",
    "    def load_credit(self):\n",
    "        df = pd.read_csv(self.credit_path)\n",
    "        # No timestamp conversion needed (Time is seconds since first transaction)\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Construct derived features for fraud data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def ip_to_int(ip_str):\n",
    "        # Use ipaddress library for robust conversion; assume IPv4 dotted decimal\n",
    "        try:\n",
    "            return int(ipaddress.ip_address(ip_str))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    def add_ip_country(self, fraud_df, ip_df):\n",
    "        # Convert ip_address to integer representation\n",
    "        fraud_df['ip_int'] = fraud_df['ip_address'].apply(self.ip_to_int)\n",
    "        # Drop rows where ip conversion failed\n",
    "        fraud_df = fraud_df.dropna(subset=['ip_int']).copy()\n",
    "        fraud_df['ip_int'] = fraud_df['ip_int'].astype(int)\n",
    "\n",
    "        # We'll do a range join: for each fraud row, find the ip_df row where lower_int <= ip_int <= upper_int\n",
    "        # Efficient approach: sort and use merge_asof then filter\n",
    "        ip_sorted = ip_df.sort_values('lower_int').reset_index(drop=True)\n",
    "        fraud_sorted = fraud_df.sort_values('ip_int').reset_index(drop=True)\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            fraud_sorted,\n",
    "            ip_sorted,\n",
    "            left_on='ip_int',\n",
    "            right_on='lower_int',\n",
    "            direction='backward',\n",
    "            suffixes=('','_ip')\n",
    "        )\n",
    "        # Filter to ensure ip_int <= upper_int (since merge_asof only guarantees lower_int <= ip_int)\n",
    "        merged = merged[merged['ip_int'] <= merged['upper_int']].copy()\n",
    "        # If some IPs didn't match, country will be NaN\n",
    "        merged.rename(columns={'country': 'ip_country'}, inplace=True)\n",
    "        return merged\n",
    "\n",
    "    @staticmethod\n",
    "    def add_time_features(fraud_df):\n",
    "        # Hour of day and day of week\n",
    "        fraud_df['hour_of_day'] = fraud_df['purchase_time'].dt.hour\n",
    "        fraud_df['day_of_week'] = fraud_df['purchase_time'].dt.day_name()\n",
    "        # Time since signup in seconds / minutes / hours\n",
    "        fraud_df['time_since_signup'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds()\n",
    "        # Replace negative or missing with NaN\n",
    "        fraud_df.loc[fraud_df['time_since_signup'] < 0, 'time_since_signup'] = np.nan\n",
    "        return fraud_df\n",
    "\n",
    "    @staticmethod\n",
    "    def add_user_velocity(fraud_df):\n",
    "        # Count previous purchases per user up to current purchase time\n",
    "        fraud_df = fraud_df.sort_values(['user_id', 'purchase_time'])\n",
    "        fraud_df['user_txn_count_so_far'] = fraud_df.groupby('user_id').cumcount()\n",
    "        # Time since last purchase\n",
    "        fraud_df['last_purchase_time'] = fraud_df.groupby('user_id')['purchase_time'].shift(1)\n",
    "        fraud_df['time_since_last_purchase'] = (fraud_df['purchase_time'] - fraud_df['last_purchase_time']).dt.total_seconds()\n",
    "        fraud_df['time_since_last_purchase'].fillna(-1, inplace=True)  # -1 indicates first purchase\n",
    "        return fraud_df\n",
    "\n",
    "# === EDA / Visualization Helpers ===\n",
    "\n",
    "class EDA:\n",
    "    \"\"\"Exploratory Data Analysis for both datasets.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_class_balance(series, title=\"Class Distribution\"):\n",
    "        counts = series.value_counts().sort_index()\n",
    "        labels = counts.index.astype(str)\n",
    "        plt.figure()\n",
    "        plt.bar(labels, counts.values)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        for i, v in enumerate(counts.values):\n",
    "            plt.text(i, v + max(counts.values)*0.01, str(v), ha='center')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_numeric_distribution(df, column, by_class='class', bins=50):\n",
    "        plt.figure()\n",
    "        for cls in sorted(df[by_class].unique()):\n",
    "            subset = df[df[by_class] == cls]\n",
    "            plt.hist(subset[column].dropna(), bins=bins, alpha=0.5, label=f\"{by_class}={cls}\", density=False)\n",
    "        plt.title(f\"Distribution of {column} by {by_class}\")\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def bar_categorical_rate(df, cat_col, target_col, top_n=10):\n",
    "        # Show fraud rate per category (only for top_n frequent categories)\n",
    "        counts = df[cat_col].value_counts().nlargest(top_n)\n",
    "        rates = []\n",
    "        for cat in counts.index:\n",
    "            subset = df[df[cat_col] == cat]\n",
    "            rate = subset[target_col].mean()\n",
    "            rates.append(rate)\n",
    "        plt.figure()\n",
    "        plt.barh([str(c) for c in counts.index][::-1], rates[::-1])\n",
    "        plt.title(f\"Fraud Rate by {cat_col} (top {top_n})\")\n",
    "        plt.xlabel(\"Fraud Rate\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def time_series_fraud_rate(df, time_col='purchase_time', freq='D', target='class'):\n",
    "        ts = df.set_index(time_col).resample(freq)[target].mean()\n",
    "        plt.figure()\n",
    "        plt.plot(ts.index, ts.values, marker='o')\n",
    "        plt.title(f\"Fraud Rate over Time ({freq} bins)\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Fraud Rate\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def heatmap_hour_day(df, target='class'):\n",
    "        pivot = df.pivot_table(index='day_of_week', columns='hour_of_day', values=target, aggfunc='mean')\n",
    "        # Reorder days for readability\n",
    "        order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "        pivot = pivot.reindex(order)\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.imshow(pivot, aspect='auto', origin='lower')\n",
    "        plt.colorbar(label=f\"Avg {target}\")\n",
    "        plt.title(f\"Fraud Rate by Day of Week and Hour\")\n",
    "        plt.xlabel(\"Hour of Day\")\n",
    "        plt.ylabel(\"Day of Week\")\n",
    "        plt.xticks(ticks=range(0,24), labels=range(0,24))\n",
    "        plt.yticks(ticks=range(len(order)), labels=order)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# === Execution: Load, preprocess, and run EDA ===\n",
    "\n",
    "# Paths - replace with actual file paths if different\n",
    "fraud_csv = \"Fraud_Data.csv\"              # e-commerce\n",
    "ip_csv = \"IpAddress_to_Country.csv\"\n",
    "credit_csv = \"creditcard.csv\"              # bank credit transactions\n",
    "\n",
    "# Instantiate loaders and engineers\n",
    "loader = DataLoader(fraud_csv, ip_csv, credit_csv)\n",
    "fe = FeatureEngineer()\n",
    "eda = EDA()\n",
    "\n",
    "# Load datasets\n",
    "fraud_df = loader.load_fraud()\n",
    "ip_df = loader.load_ip_country()\n",
    "credit_df = loader.load_credit()\n",
    "\n",
    "# Basic overview\n",
    "print(\"Fraud Data shape:\", fraud_df.shape)\n",
    "print(\"Credit Card Data shape:\", credit_df.shape)\n",
    "print(\"IP-country mapping shape:\", ip_df.shape)\n",
    "\n",
    "# === EDA on Fraud_Data.csv ===\n",
    "print(\"\\n--- Class imbalance in fraud dataset ---\")\n",
    "# The target column is named 'class' in fraud_df\n",
    "eda.plot_class_balance(fraud_df['class'], title=\"Fraud_Data.csv: class distribution\")\n",
    "\n",
    "# Feature engineering: geolocation, time, velocity\n",
    "fraud_df = fe.add_ip_country(fraud_df, ip_df)\n",
    "fraud_df = fe.add_time_features(fraud_df)\n",
    "fraud_df = fe.add_user_velocity(fraud_df)\n",
    "\n",
    "# Inspect missing after merges\n",
    "print(\"\\nMissing values (fraud_df):\")\n",
    "print(fraud_df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Visualizations\n",
    "# Numeric: purchase_value\n",
    "eda.plot_numeric_distribution(fraud_df, 'purchase_value', by_class='class', bins=40)\n",
    "\n",
    "# Categorical fraud rate: source, browser, ip_country, sex\n",
    "for cat in ['source', 'browser', 'ip_country', 'sex']:\n",
    "    if cat in fraud_df.columns:\n",
    "        eda.bar_categorical_rate(fraud_df, cat, 'class', top_n=8)\n",
    "\n",
    "# Time patterns\n",
    "eda.heatmap_hour_day(fraud_df, target='class')\n",
    "eda.time_series_fraud_rate(fraud_df, time_col='purchase_time', freq='D', target='class')\n",
    "\n",
    "# === EDA on creditcard.csv ===\n",
    "print(\"\\n--- Class imbalance in credit card dataset ---\")\n",
    "# Target is 'Class' (capitalized)\n",
    "eda.plot_class_balance(credit_df['Class'], title=\"creditcard.csv: Class distribution\")\n",
    "\n",
    "# Quick distribution of Amount by class\n",
    "eda.plot_numeric_distribution(credit_df, 'Amount', by_class='Class', bins=40)\n",
    "\n",
    "# Since credit dataset uses anonymized components (V1..V28), we can inspect correlations for top features:\n",
    "corr = credit_df.corr()\n",
    "# Example: show top correlations with Class\n",
    "corr_with_class = corr['Class'].abs().sort_values(ascending=False).drop('Class')\n",
    "print(\"\\nTop features correlated with fraud in creditcard.csv:\")\n",
    "print(corr_with_class.head(10))\n",
    "\n",
    "# === Summary Statistics ===\n",
    "def print_basic_stats(df, target_col):\n",
    "    print(f\"\\n=== Summary for target={target_col} ===\")\n",
    "    print(\"Overall count:\", len(df))\n",
    "    print(\"Positive (fraud) count:\", df[target_col].sum())\n",
    "    print(\"Negative count:\", len(df) - df[target_col].sum())\n",
    "    print(\"Fraud rate: {:.4f}\".format(df[target_col].mean()))\n",
    "\n",
    "print_basic_stats(fraud_df, 'class')\n",
    "print_basic_stats(credit_df, 'Class')\n",
    "\n",
    "# === Save cleaned intermediate EDA snapshots if desired ===\n",
    "fraud_df.to_parquet(\"processed_fraud_data.parquet\", index=False)\n",
    "credit_df.to_parquet(\"processed_credit_data.parquet\", index=False)\n"
   ],
   "id": "22469b02ef114ab9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
