{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipaddress\n",
    "import pyarrow.parquet\n",
    "from pandas.io.parquet import to_parquet\n",
    "from datetime import timedelta\n",
    "from collections import Counter\n",
    "import shap\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score,\n",
    "    f1_score, average_precision_score, classification_report\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For consistent plotting\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# === Utility / Base Classes ===\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Load and perform initial cleaning on required datasets.\"\"\"\n",
    "    def __init__(self, fraud_path, ip_path, credit_path):\n",
    "        self.fraud_path = fraud_path\n",
    "        self.ip_path = ip_path\n",
    "        self.credit_path = credit_path\n",
    "\n",
    "    def load_fraud(self):\n",
    "        df = pd.read_csv(self.fraud_path)\n",
    "        # Timestamp parsing\n",
    "        df['signup_time'] = pd.to_datetime(df['signup_time'], errors='coerce')\n",
    "        df['purchase_time'] = pd.to_datetime(df['purchase_time'], errors='coerce')\n",
    "        # Drop exact duplicates\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def load_ip_country(self):\n",
    "        ip_df = pd.read_csv(self.ip_path)\n",
    "        # Convert the bounds to integers for range matching\n",
    "        ip_df['lower_int'] = ip_df['lower_bound_ip_address'].astype(int)\n",
    "        ip_df['upper_int'] = ip_df['upper_bound_ip_address'].astype(int)\n",
    "        # Keep needed columns\n",
    "        return ip_df[['lower_int', 'upper_int', 'country']].sort_values('lower_int').reset_index(drop=True)\n",
    "\n",
    "    def load_credit(self):\n",
    "        df = pd.read_csv(self.credit_path)\n",
    "        # No timestamp conversion needed (Time is seconds since first transaction)\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Construct derived features for fraud data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def ip_to_int(ip_str):\n",
    "        # Use ipaddress library for robust conversion; assume IPv4 dotted decimal\n",
    "        try:\n",
    "            return int(ipaddress.ip_address(ip_str))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    def add_ip_country(self, fraud_df, ip_df):\n",
    "        # Convert ip_address to integer representation\n",
    "        fraud_df['ip_int'] = fraud_df['ip_address'].apply(self.ip_to_int)\n",
    "        # Drop rows where ip conversion failed\n",
    "        fraud_df = fraud_df.dropna(subset=['ip_int']).copy()\n",
    "        fraud_df['ip_int'] = fraud_df['ip_int'].astype(int)\n",
    "\n",
    "        # We'll do a range join: for each fraud row, find the ip_df row where lower_int <= ip_int <= upper_int\n",
    "        # Efficient approach: sort and use merge_asof then filter\n",
    "        ip_sorted = ip_df.sort_values('lower_int').reset_index(drop=True)\n",
    "        fraud_sorted = fraud_df.sort_values('ip_int').reset_index(drop=True)\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            fraud_sorted,\n",
    "            ip_sorted,\n",
    "            left_on='ip_int',\n",
    "            right_on='lower_int',\n",
    "            direction='backward',\n",
    "            suffixes=('','_ip')\n",
    "        )\n",
    "        # Filter to ensure ip_int <= upper_int (since merge_asof only guarantees lower_int <= ip_int)\n",
    "        merged = merged[merged['ip_int'] <= merged['upper_int']].copy()\n",
    "        # If some IPs didn't match, country will be NaN\n",
    "        merged.rename(columns={'country': 'ip_country'}, inplace=True)\n",
    "        return merged\n",
    "\n",
    "    @staticmethod\n",
    "    def add_time_features(fraud_df):\n",
    "        # Hour of day and day of week\n",
    "        fraud_df['hour_of_day'] = fraud_df['purchase_time'].dt.hour\n",
    "        fraud_df['day_of_week'] = fraud_df['purchase_time'].dt.day_name()\n",
    "        # Time since signup in seconds / minutes / hours\n",
    "        fraud_df['time_since_signup'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds()\n",
    "        # Replace negative or missing with NaN\n",
    "        fraud_df.loc[fraud_df['time_since_signup'] < 0, 'time_since_signup'] = np.nan\n",
    "        return fraud_df\n",
    "\n",
    "    @staticmethod\n",
    "    def add_user_velocity(fraud_df):\n",
    "        # Count previous purchases per user up to current purchase time\n",
    "        fraud_df = fraud_df.sort_values(['user_id', 'purchase_time'])\n",
    "        fraud_df['user_txn_count_so_far'] = fraud_df.groupby('user_id').cumcount()\n",
    "        # Time since last purchase\n",
    "        fraud_df['last_purchase_time'] = fraud_df.groupby('user_id')['purchase_time'].shift(1)\n",
    "        fraud_df['time_since_last_purchase'] = (fraud_df['purchase_time'] - fraud_df['last_purchase_time']).dt.total_seconds()\n",
    "        fraud_df['time_since_last_purchase'] = fraud_df['time_since_last_purchase'].fillna(-1)\n",
    "        # -1 indicates first purchase\n",
    "        return fraud_df\n",
    "\n",
    "# === EDA / Visualization Helpers ===\n",
    "\n",
    "class EDA:\n",
    "    \"\"\"Exploratory Data Analysis for both datasets.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_class_balance(series, title=\"Class Distribution\"):\n",
    "        counts = series.value_counts().sort_index()\n",
    "        labels = counts.index.astype(str)\n",
    "        plt.figure()\n",
    "        plt.bar(labels, counts.values)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        for i, v in enumerate(counts.values):\n",
    "            plt.text(i, v + max(counts.values)*0.01, str(v), ha='center')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    @staticmethod\n",
    "    def plot_numeric_distribution(df, column, by_class='class', bins=50):\n",
    "        plt.figure()\n",
    "        has_label = False\n",
    "        for cls in sorted(df[by_class].dropna().unique()):\n",
    "            subset = df[df[by_class] == cls]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            plt.hist(subset[column].dropna(), bins=bins, alpha=0.5, label=f\"{by_class}={cls}\", density=False)\n",
    "            has_label = True\n",
    "        plt.title(f\"Distribution of {column} by {by_class}\")\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel(\"Count\")\n",
    "        if has_label:\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bar_categorical_rate(df, cat_col, target_col, top_n=10):\n",
    "        # Show fraud rate per category (only for top_n frequent categories)\n",
    "        counts = df[cat_col].value_counts().nlargest(top_n)\n",
    "        rates = []\n",
    "        for cat in counts.index:\n",
    "            subset = df[df[cat_col] == cat]\n",
    "            rate = subset[target_col].mean()\n",
    "            rates.append(rate)\n",
    "        plt.figure()\n",
    "        plt.barh([str(c) for c in counts.index][::-1], rates[::-1])\n",
    "        plt.title(f\"Fraud Rate by {cat_col} (top {top_n})\")\n",
    "        plt.xlabel(\"Fraud Rate\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def time_series_fraud_rate(df, time_col='purchase_time', freq='D', target='class'):\n",
    "        ts = df.set_index(time_col).resample(freq)[target].mean()\n",
    "        plt.figure()\n",
    "        plt.plot(ts.index, ts.values, marker='o')\n",
    "        plt.title(f\"Fraud Rate over Time ({freq} bins)\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Fraud Rate\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def heatmap_hour_day(df, target='class'):\n",
    "        pivot = df.pivot_table(index='day_of_week', columns='hour_of_day', values=target, aggfunc='mean')\n",
    "        # Reorder days for readability\n",
    "        order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "        pivot = pivot.reindex(order)\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.imshow(pivot, aspect='auto', origin='lower')\n",
    "        plt.colorbar(label=f\"Avg {target}\")\n",
    "        plt.title(f\"Fraud Rate by Day of Week and Hour\")\n",
    "        plt.xlabel(\"Hour of Day\")\n",
    "        plt.ylabel(\"Day of Week\")\n",
    "        plt.xticks(ticks=range(0,24), labels=range(0,24))\n",
    "        plt.yticks(ticks=range(len(order)), labels=order)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "class Modelling:\n",
    "    def evaluate_model(model, X_test, y_test, name=\"model\"):\n",
    "        preds = model.predict(X_test)\n",
    "        probs = model.predict_proba(X_test)[:,1]\n",
    "        precision = precision_score(y_test, preds, zero_division=0)\n",
    "        recall = recall_score(y_test, preds, zero_division=0)\n",
    "        f1 = f1_score(y_test, preds, zero_division=0)\n",
    "        pr_auc = average_precision_score(y_test, probs)\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        print(f\"--- {name} evaluation ---\")\n",
    "        print(\"Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}, PR-AUC: {:.4f}\".format(\n",
    "            precision, recall, f1, pr_auc\n",
    "        ))\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"pr_auc\": pr_auc,\n",
    "            \"confusion_matrix\": cm\n",
    "        }\n",
    "\n",
    "# === Preprocessing function for Fraud_Data.csv ===\n",
    "    def prepare_fraud_features(df):\n",
    "        df = df.copy()\n",
    "        # Drop rows with missing target or critical time features\n",
    "        df = df.dropna(subset=['class', 'purchase_time', 'signup_time'])\n",
    "        # Categorical to encode\n",
    "        categorical = ['source', 'browser', 'sex', 'ip_country']\n",
    "        # Numeric features\n",
    "        numeric = ['purchase_value', 'time_since_signup', 'user_txn_count_so_far', 'time_since_last_purchase']\n",
    "        # Some may not exist if prior steps failed; filter\n",
    "        categorical = [c for c in categorical if c in df.columns]\n",
    "        numeric = [n for n in numeric if n in df.columns]\n",
    "\n",
    "        X = df[categorical + numeric]\n",
    "        y = df['class'].astype(int)\n",
    "\n",
    "        # Build transformer: one-hot categorical, scale numeric\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical),\n",
    "            ('num', StandardScaler(), numeric)\n",
    "        ], remainder='drop')\n",
    "\n",
    "        # SMOTE + model pipeline for imbalance\n",
    "        return X, y, preprocessor\n",
    "\n",
    "# === Preprocessing for creditcard.csv ===\n",
    "    def prepare_credit_features(df):\n",
    "        df = df.copy()\n",
    "        df = df.dropna(subset=['Class'])\n",
    "        # The anonymized features V1..V28 plus Amount, Time are usable\n",
    "        features = [c for c in df.columns if c not in ['Class']]\n",
    "        X = df[features]\n",
    "        y = df['Class'].astype(int)\n",
    "        # Scale amount/time (others from PCA already scaled-ish)\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('scale', StandardScaler(), ['Time', 'Amount'])\n",
    "        ], remainder='passthrough')  # keep V1..V28 as is\n",
    "        return X, y, preprocessor\n",
    "# === Split / Resample / Train for a generic dataset ===\n",
    "    def train_and_compare(X, y, preprocessor, random_state=42):\n",
    "        # Train-test split (stratify because of imbalance)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.25, stratify=y, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Pipeline: SMOTE -> preprocess -> classifier\n",
    "        # Logistic Regression baseline\n",
    "        pipe_lr = ImbPipeline(steps=[\n",
    "            ('smote', SMOTE(random_state=random_state)),\n",
    "            ('prep', preprocessor),\n",
    "            ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "        ])\n",
    "        pipe_lr.fit(X_train, y_train)\n",
    "        metrics_lr = evaluate_model(pipe_lr, X_test, y_test, name=\"Logistic Regression\")\n",
    "\n",
    "        # XGBoost (powerful ensemble)\n",
    "        pipe_xgb = ImbPipeline(steps=[\n",
    "            ('smote', SMOTE(random_state=random_state)),\n",
    "            ('prep', preprocessor),\n",
    "            ('clf', XGBClassifier(\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                scale_pos_weight=1,  # SMOTE already balances, so keep 1\n",
    "                n_estimators=100,\n",
    "                random_state=random_state\n",
    "            ))\n",
    "        ])\n",
    "        pipe_xgb.fit(X_train, y_train)\n",
    "        metrics_xgb = evaluate_model(pipe_xgb, X_test, y_test, name=\"XGBoost\")\n",
    "\n",
    "        return {\n",
    "            'logistic': (pipe_lr, metrics_lr),\n",
    "            'xgboost': (pipe_xgb, metrics_xgb),\n",
    "            'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test\n",
    "        }\n",
    "class Explain:\n",
    "    def select_best_model(result_dict):\n",
    "        # Compare by F1 first, then PR-AUC\n",
    "        candidates = []\n",
    "        for name in ['logistic', 'xgboost']:\n",
    "            model, metrics = result_dict[name]\n",
    "            candidates.append((name, model, metrics['f1'], metrics['pr_auc']))\n",
    "        # Sort by f1 desc, then pr_auc desc\n",
    "        candidates.sort(key=lambda x: (x[2], x[3]), reverse=True)\n",
    "        best_name, best_model, best_f1, best_pr_auc = candidates[0]\n",
    "        print(f\"Selected best model: {best_name} with F1={best_f1:.4f}, PR-AUC={best_pr_auc:.4f}\")\n",
    "        return best_name, best_model\n",
    "    def explain_with_shap(best_model, X_train, X_test, y_test, dataset_label):\n",
    "        # Determine underlying estimator (pipeline or raw)\n",
    "        # If pipeline, extract final estimator and preprocessor\n",
    "        if hasattr(best_model, 'named_steps'):\n",
    "            clf = best_model.named_steps['clf']\n",
    "            prep = best_model.named_steps.get('prep', None)\n",
    "            # Need to transform data before SHAP if prep exists\n",
    "            X_train_trans = prep.transform(X_train) if prep else X_train\n",
    "            X_test_trans = prep.transform(X_test) if prep else X_test\n",
    "            feature_names = []\n",
    "            if prep:\n",
    "                # Attempt to get transformed feature names\n",
    "                try:\n",
    "                    cat_names = prep.named_transformers_['cat'].get_feature_names_out()\n",
    "                except Exception:\n",
    "                    cat_names = []\n",
    "                num_names = []\n",
    "                if 'num' in prep.named_transformers_:\n",
    "                    num_names = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                feature_names = list(cat_names) + num_names if cat_names or num_names else None\n",
    "        else:\n",
    "            clf = best_model\n",
    "            X_train_trans = X_train\n",
    "            X_test_trans = X_test\n",
    "            feature_names = X_train.columns.tolist()\n",
    "\n",
    "        # Choose explainer\n",
    "        if hasattr(clf, 'feature_importances_') or isinstance(clf, (shap.explainers._tree.TreeExplainer,)):\n",
    "            explainer = shap.TreeExplainer(clf)\n",
    "        else:\n",
    "            # KernelExplainer is expensive; sample background\n",
    "            background = shap.sample(X_train_trans, 100, random_state=0)\n",
    "            explainer = shap.KernelExplainer(clf.predict_proba, background)\n",
    "\n",
    "        # Compute SHAP values for the positive class\n",
    "        shap_values = explainer.shap_values(X_test_trans)\n",
    "        # For binary classifiers TreeExplainer returns list; pick index 1 if list\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_vals_pos = shap_values[1]\n",
    "            expected_value = explainer.expected_value[1]\n",
    "        else:\n",
    "            shap_vals_pos = shap_values\n",
    "            expected_value = explainer.expected_value\n",
    "\n",
    "        # Summary plot (global)\n",
    "        print(f\"\\nSHAP summary plot for {dataset_label} ({best_model.__class__.__name__})\")\n",
    "        shap.summary_plot(shap_vals_pos, X_test_trans, feature_names=feature_names, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"shap_summary_{dataset_label}.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Force plot for a few examples: pick 3 random fraud and 3 non-fraud\n",
    "        os.makedirs(f\"shap_force_{dataset_label}\", exist_ok=True)\n",
    "        # Need to use JS visualization for inline; also save static via matplotlib fallback\n",
    "        shap.initjs()\n",
    "        sample_idxs = []\n",
    "        # ensure enough variety\n",
    "        fraud_idxs = np.where(y_test == 1)[0]\n",
    "        nonfraud_idxs = np.where(y_test == 0)[0]\n",
    "        rng = np.random.default_rng(42)\n",
    "        selected = []\n",
    "        for group, label in [(fraud_idxs, 1), (nonfraud_idxs, 0)]:\n",
    "            if len(group) == 0:\n",
    "                continue\n",
    "            chosen = rng.choice(group, size=min(3, len(group)), replace=False)\n",
    "            selected.extend(chosen.tolist())\n",
    "        for idx in selected:\n",
    "            single_X = X_test_trans[idx]\n",
    "            # Force plot (HTML)\n",
    "            force_plot = shap.force_plot(\n",
    "                expected_value,\n",
    "                shap_vals_pos[idx],\n",
    "                single_X,\n",
    "                feature_names=feature_names,\n",
    "                matplotlib=False\n",
    "            )\n",
    "            # Save HTML version\n",
    "            shap_html = f\"shap_force_{dataset_label}/force_{idx}.html\"\n",
    "            with open(shap_html, \"w\") as f:\n",
    "                f.write(shap.plots._force_matplotlib._repr_html_(force_plot))\n",
    "            # Also show inline if in notebook\n",
    "            display(force_plot)\n",
    "\n",
    "        # Save SHAP values for downstream use\n",
    "        joblib.dump({\n",
    "            'shap_values': shap_vals_pos,\n",
    "            'expected_value': expected_value,\n",
    "            'feature_names': feature_names,\n",
    "            'X_test_transformed': X_test_trans,\n",
    "            'y_test': y_test\n",
    "        }, f\"shap_explanation_{dataset_label}.pkl\")\n",
    "        print(f\"SHAP artifacts saved for {dataset_label}.\")\n",
    "\n",
    "        return shap_vals_pos, expected_value\n",
    "\n",
    "# === Execution: Load, preprocess, and run EDA ===\n",
    "\n",
    "# Paths - replace with actual file paths if different\n",
    "fraud_csv = \"../data/Fraud_Data.csv\"              # e-commerce\n",
    "ip_csv = \"../data/IpAddress_to_Country.csv\"\n",
    "credit_csv = \"../data/creditcard.csv\"              # bank credit transactions\n",
    "\n",
    "# Instantiate loaders and engineers\n",
    "loader = DataLoader(fraud_csv, ip_csv, credit_csv)\n",
    "fe = FeatureEngineer()\n",
    "eda = EDA()\n",
    "\n",
    "# Load datasets\n",
    "fraud_df = loader.load_fraud()\n",
    "ip_df = loader.load_ip_country()\n",
    "credit_df = loader.load_credit()\n",
    "\n",
    "# Basic overview\n",
    "print(\"Fraud Data shape:\", fraud_df.shape)\n",
    "print(\"Credit Card Data shape:\", credit_df.shape)\n",
    "print(\"IP-country mapping shape:\", ip_df.shape)\n",
    "\n",
    "# === EDA on Fraud_Data.csv ===\n",
    "print(\"\\n--- Class imbalance in fraud dataset ---\")\n",
    "# The target column is named 'class' in fraud_df\n",
    "eda.plot_class_balance(fraud_df['class'], title=\"Fraud_Data.csv: class distribution\")\n",
    "\n",
    "# Feature engineering: geolocation, time, velocity\n",
    "fraud_df = fe.add_ip_country(fraud_df, ip_df)\n",
    "fraud_df = fe.add_time_features(fraud_df)\n",
    "fraud_df = fe.add_user_velocity(fraud_df)\n",
    "\n",
    "# Inspect missing after merges\n",
    "print(\"\\nMissing values (fraud_df):\")\n",
    "print(fraud_df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Visualizations\n",
    "# Numeric: purchase_value\n",
    "eda.plot_numeric_distribution(fraud_df, 'purchase_value', by_class='class', bins=40)\n",
    "\n",
    "# Categorical fraud rate: source, browser, ip_country, sex\n",
    "for cat in ['source', 'browser', 'ip_country', 'sex']:\n",
    "    if cat in fraud_df.columns:\n",
    "        eda.bar_categorical_rate(fraud_df, cat, 'class', top_n=8)\n",
    "\n",
    "# Time patterns\n",
    "eda.heatmap_hour_day(fraud_df, target='class')\n",
    "eda.time_series_fraud_rate(fraud_df, time_col='purchase_time', freq='D', target='class')\n",
    "\n",
    "# === EDA on creditcard.csv ===\n",
    "print(\"\\n--- Class imbalance in credit card dataset ---\")\n",
    "# Target is 'Class' (capitalized)\n",
    "eda.plot_class_balance(credit_df['Class'], title=\"creditcard.csv: Class distribution\")\n",
    "\n",
    "# Quick distribution of Amount by class\n",
    "eda.plot_numeric_distribution(credit_df, 'Amount', by_class='Class', bins=40)\n",
    "\n",
    "# Since credit dataset uses anonymized components (V1..V28), we can inspect correlations for top features:\n",
    "corr = credit_df.corr()\n",
    "# Example: show top correlations with Class\n",
    "corr_with_class = corr['Class'].abs().sort_values(ascending=False).drop('Class')\n",
    "print(\"\\nTop features correlated with fraud in creditcard.csv:\")\n",
    "print(corr_with_class.head(10))\n",
    "\n",
    "# === Summary Statistics ===\n",
    "def print_basic_stats(df, target_col):\n",
    "    print(f\"\\n=== Summary for target={target_col} ===\")\n",
    "    print(\"Overall count:\", len(df))\n",
    "    print(\"Positive (fraud) count:\", df[target_col].sum())\n",
    "    print(\"Negative count:\", len(df) - df[target_col].sum())\n",
    "    print(\"Fraud rate: {:.4f}\".format(df[target_col].mean()))\n",
    "\n",
    "print_basic_stats(fraud_df, 'class')\n",
    "print_basic_stats(credit_df, 'Class')\n",
    "\n",
    "# === Save cleaned intermediate EDA snapshots if desired ===\n",
    "fraud_df.to_parquet(\"../data/processed_fraud_data.parquet\", index=False)\n",
    "credit_df.to_parquet(\"../data/processed_credit_data.parquet\", index=False)\n",
    "\n",
    "fraud_df.to_csv(\"../data/processed_fraud_data.parquet\", index=False)\n",
    "credit_df.to_csv(\"../data/processed_credit_data.parquet\", index=False)\n",
    "\n",
    "mod = Modelling()\n",
    "# === Run on Fraud_Data.csv ===\n",
    "X_fraud, y_fraud, pre_fraud = mod.prepare_fraud_features(fraud_df)\n",
    "results_fraud = mod.train_and_compare(X_fraud, y_fraud, pre_fraud)\n",
    "\n",
    "# === Run on creditcard.csv ===\n",
    "X_credit, y_credit, pre_credit = mod.prepare_credit_features(credit_df)\n",
    "results_credit = mod.train_and_compare(X_credit, y_credit, pre_credit)\n",
    "\n",
    "ex = Explain()\n",
    "\n",
    "# === Apply to fraud dataset ===\n",
    "best_name_fraud, best_model_fraud = ex.select_best_model(results_fraud)\n",
    "shap_vals_fraud, explainer_expected_fraud = ex.explain_with_shap(\n",
    "    best_model_fraud,\n",
    "    results_fraud['X_train'],\n",
    "    results_fraud['X_test'],\n",
    "    results_fraud['y_test'],\n",
    "    dataset_label=\"fraud\"\n",
    ")\n",
    "\n",
    "# === Apply to credit card dataset ===\n",
    "best_name_credit, best_model_credit = ex.select_best_model(results_credit)\n",
    "shap_vals_credit, explainer_expected_credit = ex.explain_with_shap(\n",
    "    best_model_credit,\n",
    "    results_credit['X_train'],\n",
    "    results_credit['X_test'],\n",
    "    results_credit['y_test'],\n",
    "    dataset_label=\"credit\"\n",
    ")\n"
   ],
   "id": "22469b02ef114ab9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
